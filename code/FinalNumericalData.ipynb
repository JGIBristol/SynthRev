{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create the final numerical data set.\n",
    "\n",
    "The code extracts rows with the fewest NaN values\n",
    "\n",
    "The code runs through 16 iterations to ensure the whole data set is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the items table\n",
    "def load_items(file_path):\n",
    "    \"\"\"\n",
    "    Loads the items data from a CSV file and filters it based on specific item IDs.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the items CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A filtered DataFrame containing specific items.\n",
    "    \"\"\"\n",
    "    items = pd.read_csv(file_path, usecols=['itemid', 'label', 'abbreviation'])\n",
    "    filtered_items = items[items['itemid'].isin([220210, 220277, 225309, 220045, 220739, 223900, 223901, 223762])]\n",
    "    return filtered_items\n",
    "\n",
    "# Function to load and filter the chart events data in chunks\n",
    "def load_chart_events_in_chunks(file_path, chunk_size=20000000):\n",
    "    \"\"\"\n",
    "    Loads the chart events data from a CSV file in chunks and filters it based on specific item IDs.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the chart events CSV file.\n",
    "    chunk_size (int): The number of rows per chunk to read from the CSV file.\n",
    "\n",
    "    Yields:\n",
    "    pd.DataFrame: A filtered DataFrame containing specific chart events for each chunk.\n",
    "    \"\"\"\n",
    "    chunk_iter = pd.read_csv(file_path, usecols=['subject_id', 'hadm_id', 'charttime', 'itemid', 'valuenum', 'valueuom'], chunksize=chunk_size)\n",
    "    for chunk in chunk_iter:\n",
    "        filtered_chunk = chunk[chunk['itemid'].isin([220210, 220277, 225309, 220045, 220739, 223900, 223901, 223762])]\n",
    "        yield filtered_chunk\n",
    "\n",
    "# Function to combine the two datasets\n",
    "def combine_data(chart, items):\n",
    "    \"\"\"\n",
    "    Combines the chart events and items data into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    chart (pd.DataFrame): The chart events DataFrame.\n",
    "    items (pd.DataFrame): The items DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The combined DataFrame.\n",
    "    \"\"\"\n",
    "    combined_data = pd.merge(chart, items, on=['itemid'])\n",
    "    combined_data = combined_data.pivot_table(index=['subject_id', 'hadm_id', 'charttime'], columns='label', values='valuenum').reset_index()\n",
    "    combined_data['GCS Total'] = combined_data[['GCS - Eye Opening', 'GCS - Motor Response', 'GCS - Verbal Response']].sum(axis=1)\n",
    "    combined_data['GCS Total'].replace(0, np.nan, inplace=True)\n",
    "    combined_data = combined_data.drop(columns=['GCS - Eye Opening', 'GCS - Motor Response', 'GCS - Verbal Response'])\n",
    "    return combined_data\n",
    "\n",
    "# Function to select the chart time with the fewest NaNs\n",
    "def option_fewest_nans(data):\n",
    "    \"\"\"\n",
    "    Selects the chart time with the fewest NaNs for each hadm_id.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The combined DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the fewest NaNs for each hadm_id.\n",
    "    \"\"\"\n",
    "    data['nan_count'] = data.isna().sum(axis=1)\n",
    "    sorted_data = data.sort_values(by=['hadm_id', 'nan_count', 'charttime'])\n",
    "    fewest_nans_data = sorted_data.drop_duplicates(subset=['hadm_id'], keep='first').drop(columns=['nan_count'])\n",
    "    return fewest_nans_data\n",
    "\n",
    "# Function to load patient data\n",
    "def load_patient_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the patient data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the patient CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the patient data.\n",
    "    \"\"\"\n",
    "    patient_data = pd.read_csv(file_path, usecols=['subject_id', 'anchor_age'])\n",
    "    return patient_data\n",
    "\n",
    "# Function to merge patient data with another DataFrame\n",
    "def merge_with_patient_data(patient_data, df):\n",
    "    \"\"\"\n",
    "    Merges the patient data with another DataFrame based on the 'subject_id' column.\n",
    "\n",
    "    Parameters:\n",
    "    patient_data (pd.DataFrame): The DataFrame containing patient data.\n",
    "    df (pd.DataFrame): The DataFrame to be merged with patient data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged DataFrame.\n",
    "    \"\"\"\n",
    "    final_df = pd.merge(patient_data, df, on=['subject_id'])\n",
    "    return final_df\n",
    "\n",
    "# Function to process the data in chunks and output the results\n",
    "def process_and_output_chunks(chart_file_path, items_file_path, patient_file_path, chunk_size=20000000):\n",
    "    \"\"\"\n",
    "    Processes the chart events data in chunks, combines it with items and patient data, \n",
    "    and outputs the results while ensuring no duplicates within each chunk.\n",
    "\n",
    "    Parameters:\n",
    "    chart_file_path (str): The path to the chart events CSV file.\n",
    "    items_file_path (str): The path to the items CSV file.\n",
    "    patient_file_path (str): The path to the patient data CSV file.\n",
    "    chunk_size (int): The number of rows per chunk to read from the chart events CSV file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of processed DataFrames.\n",
    "    \"\"\"\n",
    "    items = load_items(items_file_path)  # Load and filter items data\n",
    "    patient_data = load_patient_data(patient_file_path)  # Load patient data\n",
    "    chunk_number = 0\n",
    "    final_dataframes = []  # List to store processed DataFrames\n",
    "\n",
    "    # Iterate over each chunk of chart events data\n",
    "    for chart_chunk in load_chart_events_in_chunks(chart_file_path, chunk_size):\n",
    "        combined_data = combine_data(chart_chunk, items)  # Combine chart events and items data\n",
    "        processed_data = option_fewest_nans(combined_data)  # Select rows with the fewest NaNs\n",
    "        final_data = merge_with_patient_data(patient_data, processed_data)  # Merge with patient data\n",
    "\n",
    "        # Ensure no duplicates within the chunk\n",
    "        final_data = final_data.drop_duplicates()\n",
    "\n",
    "        final_dataframes.append(final_data)  # Store the processed DataFrame\n",
    "        \n",
    "        # Output the processed data\n",
    "        print(f\"Processed chunk {chunk_number}\")\n",
    "        #print(final_data.head())  # Print the first few rows of the processed data\n",
    "        \n",
    "        chunk_number += 1\n",
    "    \n",
    "    return final_dataframes\n",
    "\n",
    "# Function to combine all DataFrames and clean the resulting DataFrame\n",
    "def combine_and_clean_dataframes(dataframes):\n",
    "    \"\"\"\n",
    "    Combines all DataFrames into one large DataFrame, removes duplicates based on hadm_id, \n",
    "    and removes rows with more than 2 NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list): List of DataFrames to be combined and cleaned.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The combined and cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Combine all DataFrames into one large DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Drop duplicates from the combined DataFrame based on hadm_id\n",
    "    combined_df = combined_df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "    # Remove rows with more than 2 NaN values\n",
    "    cleaned_df = combined_df.dropna(thresh=len(combined_df.columns) - 2)\n",
    "    \n",
    "    # Output the shape of the final cleaned DataFrame\n",
    "    print(\"Combined and cleaned DataFrame shape:\", cleaned_df.shape)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# Specify the paths to your large CSV files\n",
    "chart_file_path = os.path.join('..', 'chartevents.csv')\n",
    "items_file_path = os.path.join('..', 'd_items.csv')\n",
    "patient_file_path = os.path.join('..','patients.csv', 'patients.csv')\n",
    "\n",
    "# Call the function to process the files\n",
    "final_dataframes = process_and_output_chunks(chart_file_path, items_file_path, patient_file_path)\n",
    "\n",
    "# Combine and clean the final DataFrames\n",
    "cleaned_data = combine_and_clean_dataframes(final_dataframes)\n",
    "\n",
    "cleaned_data.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
