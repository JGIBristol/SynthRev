{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a GAN based of the example from this blog post: https://blog.paperspace.com/implementing-gans-in-tensorflow/\n",
    "\n",
    "Working with a single column in the data for experimental purposes. (i.e. Heart Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data ie load only the heart rate column and use 10,000 observations\n",
    "data = pd.read_csv('final.csv', usecols= ['Heart Rate'], nrows= 10000)\n",
    "\n",
    "data = data.values # Convert to numpy array for TensorFlow compatibility\n",
    "\n",
    "# Normalize the data \n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Define batch size - number of training samples used in a single iteration\n",
    "batch_size = 256\n",
    "\n",
    "# Function to randomly select a batch of samples from the dataset for training.\n",
    "def sample_data(n= batch_size):\n",
    "    \"\"\"\n",
    "    Inputs n: The number of samples to be drawn in a single batch. It defaults to batch_size.\n",
    "\n",
    "    Uses np.random.randint to generate n random integers between 0 and the total number of \n",
    "    samples in the dataset (data.shape[0])\n",
    "\n",
    "    Purpose: provide a random batch of samples from the dataset during each training iteration. \n",
    "    This randomness helps in training the model more effectively by exposing it to different parts of the dataset in each iteration, \n",
    "    promoting better generalization and reducing overfitting.\n",
    "\n",
    "    \"\"\"\n",
    "    indices = np.random.randint(0, data.shape[0], n)\n",
    "    return data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "def generator(Z, output_dim=1, hsize=[64, 32, 16], reuse=False):\n",
    "    \"\"\" \n",
    "    Inputs: Z - placeholder for random input samples, output_dim - desired dimensionality of the output,\n",
    "    hsize - List defining the number of units in hidden layers, reuse - Boolean to reuse the same layers\n",
    "\n",
    "    Layers: h1 - first hidden layer with leaky ReLU activation, followed by batch normalization and dropout,\n",
    "            h2 - second hidden layer with leaky ReLU activation, followed by batch normalization and dropout,\n",
    "            h3 - third hidden layer with leaky ReLU activation, followed by batch normalization and dropout,\n",
    "            out - output layer generating a vector with the desired dimensionality\n",
    "\n",
    "    This function creates a fully connected neural network with three hidden layers and outputs a vector,\n",
    "    matching the specified dimensions of the real dataset. The goal is for the generator to learn the distribution of the real data.\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(\"GAN/Generator\", reuse=reuse):\n",
    "        h1 = tf.keras.layers.Dense(hsize[0], activation=None, kernel_initializer=tf.keras.initializers.he_normal())(Z)\n",
    "        h1 = tf.keras.layers.BatchNormalization()(h1)\n",
    "        h1 = tf.nn.leaky_relu(h1)\n",
    "        h1 = tf.keras.layers.Dropout(0.3)(h1)  # Applying dropout with a 30% rate\n",
    "\n",
    "        h2 = tf.keras.layers.Dense(hsize[1], activation=None, kernel_initializer=tf.keras.initializers.he_normal())(h1)\n",
    "        h2 = tf.keras.layers.BatchNormalization()(h2)\n",
    "        h2 = tf.nn.leaky_relu(h2)\n",
    "        h2 = tf.keras.layers.Dropout(0.3)(h2)  # Applying dropout with a 30% rate\n",
    "\n",
    "        h3 = tf.keras.layers.Dense(hsize[2], activation=None, kernel_initializer=tf.keras.initializers.he_normal())(h2)\n",
    "        h3 = tf.keras.layers.BatchNormalization()(h3)\n",
    "        h3 = tf.nn.leaky_relu(h3)\n",
    "        h3 = tf.keras.layers.Dropout(0.3)(h3)  # Applying dropout with a 30% rate\n",
    "\n",
    "        out = tf.keras.layers.Dense(output_dim, activation=tf.nn.sigmoid, kernel_initializer=tf.keras.initializers.glorot_uniform())(h3)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "def discriminator(X, hsize=[16], reuse=False):\n",
    "    \"\"\"\n",
    "    Inputs: X - placeholder for input samples (real or generated), hsize - List defining the number of units in hidden layers\n",
    "    reuse - Boolean to reuse the same layers\n",
    "\n",
    "    Layers: h1 - hidden layer with leaky ReLU activation,\n",
    "    out - Output layer generating a logit prediction.\n",
    "\n",
    "    The discriminator evaluates whether the input samples are real or generated. The output consists of \n",
    "    logit prediction: indicates the probability of the input being real and h1 Output: Feature transformation learned by the discriminator.\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(\"GAN/Discriminator\", reuse=reuse):\n",
    "        h1 = tf.keras.layers.Dense(hsize[0], activation=tf.nn.leaky_relu)(X)\n",
    "        h1 = tf.keras.layers.Dropout(0.5)(h1)  # Applying dropout with a 50% rate\n",
    "        out = tf.keras.layers.Dense(1)(h1)\n",
    "\n",
    "    return out, h1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paceholders X and Z for real samples and random noise\n",
    "tf.compat.v1.disable_eager_execution() \n",
    "\n",
    "num_features = data.shape[1] # Determines number of features in the data\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, num_features]) # X: placeholder for real data\n",
    "Z = tf.compat.v1.placeholder(tf.float32, [None, 100]) # Z: placeholder for random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph for generating samples from Generator Network and feeding real and generated data to Discriminator\n",
    "\"\"\"\n",
    "G_sample : Output from the Generator network, generated from random noise (Z)\n",
    "r_logits : Logit predictions for real samples X\n",
    "r_rep : Feature representation from the Discriminator for real samples.\n",
    "f_logits : Logit predictions for generated samples G_sample\n",
    "g_rep : Feature representation from the Discriminator for generated samples\n",
    "\n",
    "This set up creates a computational graph where:\n",
    "\n",
    "** The Generator creates fake samples from random noise.\n",
    "** The Discriminator evaluates both real and fake samples.\n",
    "** The Discriminator's parameters are reused to ensure consistent training.\n",
    "\"\"\"\n",
    "\n",
    "# Generate samples from the Generator network\n",
    "G_sample = generator(Z)\n",
    "\n",
    "# Get logits and feature representation for real samples from the Discriminator network\n",
    "r_logits, r_rep = discriminator(X)\n",
    "\n",
    "# Get logits and feature representation for generated samples, reusing the Discriminator network\n",
    "f_logits, g_rep = discriminator(G_sample, reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions for generator and discriminator\n",
    "\"\"\"\n",
    "Discriminator loss: \n",
    "- Compares real samples' logits (r_logits) to a tensor of ones (indicating real samples).\n",
    "- Compares generated samples' logits (f_logits) to a tensor of zeros (indicating fake samples).\n",
    "- The mean of both losses represents how well the Discriminator distinguishes real from fake samples.\n",
    "\n",
    "Generator loss:\n",
    "- Compares generated samples' logits (f_logits) to a tensor of ones \n",
    "(indicating the Generator's goal to fool the Discriminator into thinking the samples are real).\n",
    "-  The mean of this loss measures how well the Generator fools the Discriminator.\n",
    "\"\"\"\n",
    "# Discriminator Loss\n",
    "disc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = r_logits, labels = tf.ones_like(r_logits))+ \n",
    "                           tf.nn.sigmoid_cross_entropy_with_logits(logits = f_logits, labels = tf.zeros_like(f_logits)))\n",
    "\n",
    "# Generator Loss\n",
    "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = f_logits, labels = tf.ones_like(f_logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimisers for the networks using RMSProprOptimizer\n",
    "\"\"\"\n",
    "Purpose: Optimizers are crucial for updating the network weights to minimize the loss functions.\n",
    "By specifying the var_list, we ensure that each optimizer only updates the variables of the respective network.\n",
    "\n",
    "This setup allows the GAN to train both networks in an adversarial manner, \n",
    "improving the Generator's ability to create realistic data and the Discriminator's ability to distinguish real from fake data.\n",
    "\"\"\"\n",
    "# Fetch variables for Generator and Discriminator\n",
    "gen_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope = \"GAN/Generator\")\n",
    "disc_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope = \"GAN/Discriminator\")\n",
    "\n",
    "# Define RMSProp Optimizer for both networks\n",
    "gen_step = tf.compat.v1.train.AdamOptimizer(learning_rate = 0.001).minimize(gen_loss, var_list = gen_vars) # G Train step\n",
    "disc_step = tf.compat.v1.train.AdamOptimizer(learning_rate = 0.00001).minimize(disc_loss, var_list = disc_vars) # D Train step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\t Discriminator loss: 1.2077\t Generator loss: 0.7543\n",
      "Iterations: 1000\t Discriminator loss: 1.3165\t Generator loss: 0.7163\n",
      "Iterations: 2000\t Discriminator loss: 1.2582\t Generator loss: 0.7419\n",
      "Iterations: 3000\t Discriminator loss: 1.1977\t Generator loss: 0.7699\n",
      "Iterations: 4000\t Discriminator loss: 1.1381\t Generator loss: 0.8003\n",
      "Iterations: 5000\t Discriminator loss: 1.0779\t Generator loss: 0.8329\n",
      "Iterations: 6000\t Discriminator loss: 1.0171\t Generator loss: 0.8679\n",
      "Iterations: 7000\t Discriminator loss: 0.9637\t Generator loss: 0.9052\n",
      "Iterations: 8000\t Discriminator loss: 0.9050\t Generator loss: 0.9451\n",
      "Iterations: 9000\t Discriminator loss: 0.8512\t Generator loss: 0.9874\n",
      "Iterations: 10000\t Discriminator loss: 0.7976\t Generator loss: 1.0323\n",
      "Iterations: 11000\t Discriminator loss: 0.7442\t Generator loss: 1.0799\n",
      "Iterations: 12000\t Discriminator loss: 0.6944\t Generator loss: 1.1302\n",
      "Iterations: 13000\t Discriminator loss: 0.6422\t Generator loss: 1.1833\n",
      "Iterations: 14000\t Discriminator loss: 0.6002\t Generator loss: 1.2392\n",
      "Iterations: 15000\t Discriminator loss: 0.5523\t Generator loss: 1.2980\n"
     ]
    }
   ],
   "source": [
    "# Train both the networks in an alternating way.\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.compat.v1.global_variables_initializer().run(session = sess)\n",
    "\n",
    "d_steps = 1\n",
    "g_steps = 15\n",
    "\n",
    "for i in range(15001):\n",
    "\n",
    "    # Train the Discriminator\n",
    "    for _ in range(d_steps):\n",
    "        X_batch = sample_data(n=batch_size)  # Fetch real data samples\n",
    "        Z_batch = np.random.uniform(-1, 1, size=[batch_size, 100])  # Generate random noise\n",
    "        _, dloss = sess.run([disc_step, disc_loss], feed_dict={X: X_batch, Z: Z_batch})\n",
    "\n",
    "    # Train the Generator\n",
    "    for _ in range(g_steps):\n",
    "        Z_batch = np.random.uniform(-1, 1, size=[batch_size, 100])  # Generate random noise\n",
    "        _, gloss = sess.run([gen_step, gen_loss], feed_dict={Z: Z_batch})\n",
    "\n",
    "    if i % 1000 == 0:  # Print every 1000 iterations\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\" % (i, dloss, gloss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
